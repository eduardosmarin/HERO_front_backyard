---
title: "EVST 4960 Data Wrangling"
subtitle: "Creating the Final Product for the Data Analysis (V2)"
author: "Eduardo Marin"
date: "`r format(Sys.time())`"
output:
  pFinal_Giveaway_Y2017_Y2024_document:
    toc: true
  html_document:
    theme: flatly
    code_folding: hide
    fig_width: 8
    fig_height: 7
    fig_caption: true
    toc: true
    toc_float: true
    self_contained: true
editor_options:
  chunk_output_type: console
  markdown:
    wrap: 72
---

# 0. Loading the packages
```{r 1. Loading Packages, message=FALSE, include=FALSE}
# 1. Loading all packages

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,   # <--- this suppresses warnings
  message = FALSE    # <--- this suppresses messages
)

packs <-c(
            'janitor'    # cleans things up, also pipe-friendly cross-tabulations
           , 'sf'         # for spatial data support
          , 'tidyverse'  # cuz
          , 'tidylog'    # prints out what was done in dplyr and tidyr
          , 'magrittr'   # for the pipe
          , 'mapview'    # web maps for zooming and panning around
          #, 'beepr'      # makes noise when things are done!
          , 'tictoc'     # timing things.
          , 'raster'
          # , 'doParallel' # does what is says! PARALLEL
          # 'broom.mixed',# tidiers for mixed models AND nlme::gls()
          # , 'lubridate'   # DATES!
          , 'tidycensus' # tidy census package
          , 'tidygeocoder' # geo coding
          , 'leaflet' #creating the interactive mapping elements (more specific)
          , 'shiny'
          , 'leafsync'  # linked maps
          , 'openxlsx'
          , 'readxl'
          )     

#2. If the packages in 'packs' are not already installed, install them
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packs, rownames(installed.packages())))
}
vapply(packs, library, character.only = TRUE, logical(1), logical.return = TRUE, quietly = TRUE)

# 3. Items for tidy census
census_api_key('58fc555c77c229747ade7d9fe50e7c71297cf91a', install = TRUE, overwrite = TRUE)
readRenviron("~/.Renviron")
options(tigris_use_cache = TRUE)
```

Outline of objectives
 1. Home values
 2. Census
 3. General Tree Giveaways
 4. iTree
 5. Final combinination
 6. Testings
 7. Archives

# 1. Property Data: Downloading, cleaning, and subsetting property data for Holyoke, MA and Chelsea, MA (2019 and 2023)
```{r Property Data for Holyoke, MA and Chelsea, MA, echo=FALSE, message=FALSE}
# --- Running Notes --- #
# The data was downloaded from ArcGIS using the Table to Excel tool since it was difficult to read the      additional attribute table using SF in R
# For the inflation data, U.S. Bureau Labor of Statistics was used, particularly from the years 2019 and    2023 to be able to get the value needed for comparisons:           https://www.bls.gov/regions/northeast/data/consumerpriceindex_boston_table.htm
# The Unique_ID for these parcels is the Location_ID. Since the data is being saved out for each individual year, the assumption was that there should be no worries about unification or subdivision of lots, especially since the join will be year and city separate
# --------------------- #

# 1. Cleaning | Creating the standardized names to apply for the dataset
Standard_Names <- c(
  "Unique_ID","Property_ID","Location_ID","Building_Value","Land_Value","Other_Value","Total_Value",
  "Year","Lot_Size","Lot_Date","Lot_Price","Use_Code","Site_Address","Address_Number",
  "Full_Street","Location","City","Zip","OWNER1","OWN_ADDR","OWN_CITY",
  "OWN_STATE","OWN_ZIP","OWN_CO","LS_BOOK","LS_PAGE","REG_ID","Zoning",
  "Year_Built","Building_Area","Units","Residential_Area","Style","Stories","Number_Rooms",
  "Lot_Units","CAMA_ID","Town_ID"
)

# 2. Spatial & Categorical Data | Downloading the categorical and spatial data
## a. Creating the Chelsea (2019) Shapefile #LEAVE AS IS
Chelsea_Property_Data_Append_Y2019 <-
    read_excel("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Assessments_All_Table_to_Excel.xlsx"
            , sheet = "Chelsea_2019"
            , col_types = "text") |> #I need to make the col_type as text because I was having some issues from extracting the data into usable data | Rename to 2019 the value and get rid of the inflation adjusting
    setNames(Standard_Names) |>
    tidylog::select(1:18, 28:36,38) |>
    mutate(across(c(Building_Value, Land_Value, Other_Value, Total_Value,
                  Lot_Size, Lot_Price, Year_Built, Building_Area,
                  Residential_Area, Units, Number_Rooms, Stories),
                as.double)) |>
    mutate(across(Lot_Date, ymd)) |>
    mutate( 
      Building_Value = Building_Value
    , Land_Value = Land_Value
    , Other_Value = Other_Value
    , Total_Value = Total_Value
    , Lot_Price = Lot_Price
    ) |>
  mutate(Year_Dataset = "2019") |>
  rename_with(~ paste0("Parcel_", .x)) |>
  mutate(Time_Period = "Time 1") |>
  tidylog::select(Parcel_Unique_ID, Parcel_Property_ID, Parcel_Location_ID, Parcel_Lot_Size, Parcel_Lot_Date, Parcel_Lot_Units, Parcel_Lot_Price, Parcel_Building_Value, Parcel_Land_Value, Parcel_Other_Value, Parcel_Total_Value, Parcel_Year, Parcel_Style, Parcel_Use_Code, Parcel_Zoning, Parcel_Year_Built, Parcel_Building_Area, Parcel_Residential_Area, Parcel_Units, Parcel_Number_Rooms, Parcel_Stories, Time_Period, Parcel_Year_Dataset)
      
Chelsea_Property_Data_Y2019 <-
  st_read("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Chelsea_CY19_FY19/M057TaxPar_CY19_FY19.shp") |>
  rename(Parcel_Location_ID = LOC_ID) |>
  tidylog::select(4, 13) |>
  left_join(Chelsea_Property_Data_Append_Y2019, by = "Parcel_Location_ID")  |>
  st_transform(crs = 26986)

## b. Creating the Chelsea (2023) Shapefile

Chelsea_Property_Data_Append_Y2023 <-
    read_excel("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Assessments_All_Table_to_Excel.xlsx"
            , sheet = "Chelsea_2023"
            , col_types = "text") |> #I need to make the col_type as text because I was having some issues from extracting the data into usable data 
    setNames(Standard_Names) |>
    tidylog::select(1:18, 28:36,38) |>
    mutate(across(c(Building_Value, Land_Value, Other_Value, Total_Value,
                  Lot_Size, Lot_Price, Year_Built, Building_Area,
                  Residential_Area, Units, Number_Rooms, Stories),
                as.double)) |>
    mutate( 
      Building_Value = Building_Value*(281.082/326.016)
    , Land_Value = Land_Value*(281.082/326.016)
    , Other_Value = Other_Value*(281.082/326.016)
    , Total_Value = Total_Value*(281.082/326.016)
    , Lot_Price = Lot_Price*(281.082/326.016)
    ) |>
# The value that I will do is *(326.016/281.082) #Update to this
    mutate(across(Lot_Date, ymd)) |>
    mutate(Year_Dataset = "2023") |>
   rename_with(~ paste0("Parcel_", .x)) |>
    mutate(Time_Period = "Time 2") |>
  tidylog::select(Parcel_Unique_ID, Parcel_Property_ID, Parcel_Location_ID, Parcel_Lot_Size, Parcel_Lot_Date, Parcel_Lot_Units, Parcel_Lot_Price, Parcel_Building_Value, Parcel_Land_Value, Parcel_Other_Value, Parcel_Total_Value, Parcel_Year, Parcel_Style, Parcel_Use_Code, Parcel_Zoning, Parcel_Year_Built, Parcel_Building_Area, Parcel_Residential_Area, Parcel_Units, Parcel_Number_Rooms, Parcel_Stories, Time_Period, Parcel_Year_Dataset)

Chelsea_Property_Data_Y2023 <-
  st_read("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Chelsea_CY23_FY24/M057TaxPar_CY23_FY24.shp") |>
  rename(Parcel_Location_ID = LOC_ID) |>
  tidylog::select(4, 13) |>
  left_join(Chelsea_Property_Data_Append_Y2023, by = "Parcel_Location_ID") |>
  st_transform(crs = 26986)
  
  
## c. Creating the Holyoke (2019) Shapefile
Holyoke_Property_Data_Append_Y2019 <-
    read_excel("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Assessments_All_Table_to_Excel.xlsx"
            , sheet = "Holyoke_2019"
            , col_types = "text") |> #I need to make the col_type as text because I was having some issues from extracting the data into usable data 
    setNames(Standard_Names) |>
    tidylog::select(1:18, 28:36,38) |>
    mutate(across(c(Building_Value, Land_Value, Other_Value, Total_Value,
                  Lot_Size, Lot_Price, Year_Built, Building_Area,
                  Residential_Area, Units, Number_Rooms, Stories),
                as.double)) |>
   mutate( 
      Building_Value = Building_Value
    , Land_Value = Land_Value
    , Other_Value = Other_Value
    , Total_Value = Total_Value
    , Lot_Price = Lot_Price
    ) |>
    mutate(across(Lot_Date, ymd)) |>
    mutate(Year_Dataset = "2019") |>
    mutate( 
      Building_Value = Building_Value # <- The value from inflation
    , Land_Value = Land_Value #move the new stuff 
    , Other_Value = Other_Value
    , Total_Value = Total_Value
    , Lot_Price = Lot_Price
    ) |>
  rename_with(~ paste0("Parcel_", .x)) |>
  mutate(Time_Period = "Time 1") |>
  tidylog::select(Parcel_Unique_ID, Parcel_Property_ID, Parcel_Location_ID, Parcel_Lot_Size, Parcel_Lot_Date, Parcel_Lot_Units, Parcel_Lot_Price, Parcel_Building_Value, Parcel_Land_Value, Parcel_Other_Value, Parcel_Total_Value, Parcel_Year, Parcel_Style, Parcel_Use_Code, Parcel_Zoning, Parcel_Year_Built, Parcel_Building_Area, Parcel_Residential_Area, Parcel_Units, Parcel_Number_Rooms, Parcel_Stories, Time_Period, Parcel_Year_Dataset)
      
Holyoke_Property_Data_Y2019 <-
  st_read("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Holyoke_CY19_FY20/M137TaxPar_CY19_FY20.shp") |>
  rename(Parcel_Location_ID = LOC_ID) |>
  tidylog::select(4, 13) |>
  left_join(Holyoke_Property_Data_Append_Y2019, by = "Parcel_Location_ID")  |>
  st_transform(crs = 26986)

## d. Creating the Holyoke (2023) Shapefile
Holyoke_Property_Data_Append_Y2023 <-
    read_excel("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Assessments_All_Table_to_Excel.xlsx"
            , sheet = "Holyoke_2023"
            , col_types = "text") |> #I need to make the col_type as text because I was having some issues from extracting the data into usable data 
   setNames(Standard_Names) |>
    tidylog::select(1:18, 28:36,38) |>
    mutate(across(c(Building_Value, Land_Value, Other_Value, Total_Value,
                  Lot_Size, Lot_Price, Year_Built, Building_Area,
                  Residential_Area, Units, Number_Rooms, Stories),
                as.double)) |>
     mutate( 
      Building_Value = Building_Value*(281.082/326.016)
    , Land_Value = Land_Value*(281.082/326.016)
    , Other_Value = Other_Value*(281.082/326.016)
    , Total_Value = Total_Value*(281.082/326.016)
    , Lot_Price = Lot_Price*(281.082/326.016)
    ) |>
    mutate(across(Lot_Date, ymd)) |>
    mutate(Year_Dataset = "2023") |>
    rename_with(~ paste0("Parcel_", .x)) |>
    mutate(Time_Period = "Time 2") |>
  tidylog::select(Parcel_Unique_ID, Parcel_Property_ID, Parcel_Location_ID, Parcel_Lot_Size, Parcel_Lot_Date, Parcel_Lot_Units, Parcel_Lot_Price, Parcel_Building_Value, Parcel_Land_Value, Parcel_Other_Value, Parcel_Total_Value, Parcel_Year, Parcel_Style, Parcel_Use_Code, Parcel_Zoning, Parcel_Year_Built, Parcel_Building_Area, Parcel_Residential_Area, Parcel_Units, Parcel_Number_Rooms, Parcel_Stories, Time_Period, Parcel_Year_Dataset)

Holyoke_Property_Data_Y2023 <-
  st_read("../HERO_front_backyard_data_to_big/Parcel_Data_Sites_Years/Parcel_Data_Holyoke_CY23_FY23/M137TaxPar_CY23_FY23.shp") |>
  rename(Parcel_Location_ID = LOC_ID) |>
  tidylog::select(4, 13) |>
  left_join(Holyoke_Property_Data_Append_Y2023, by = "Parcel_Location_ID") |>
  st_transform(crs = 26986)

```

# 2. Census Data: Downloading and cleaning for Holyoke, MA and Chelsea, MA (2017 and 2023)
```{r Census Data for Holyoke, MA and Chelsea, MA, echo=FALSE, message=FALSE}
# 0. Archived: Census and Address Code for Geopackages # Refer to this

# 1. Reading CBGs in 2022 for General Analysis
Massachusetts_CBG_Data_Y2022 <-
  st_read("input_data/General/Massachusetts_CBG_Data_Y2022.gpkg")

# 2. Reading Holyoke, MA and Chelsea, MA boundaries 
## a. Reading the Holyoke boundary
Holyoke_Boundary <-
  st_read("input_data/Holyoke/Boundaries/holyoke_boundary.gpkg") |>
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022)) # They both have the same projections so it doesn't matter

## b. Reading the Chelsea boundary
Chelsea_Boundary <-
  st_read("input_data/Chelsea/Boundaries/chelsea_boundary.gpkg") |>
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022)) # Likewise, they both have the same projections so it shouldn't matter

#-# ESM to DHL: Technically, no need to this separaetly for each year for the CBGs. However, to not break the code, decided to keep as is for simplicity sake. 

# 3. Sub selecting the Holyoke, MA and Chelsea, MA specific block 
## b. Figuring this out for Chelsea Y2017
Chelsea_CBG_Data_Y2017 <-
  Massachusetts_CBG_Data_Y2022[
  st_intersects(Massachusetts_CBG_Data_Y2022, Chelsea_Boundary, sparse = FALSE)[, 1], ] |>
  filter(!Census_GEOID %in% c("250173424023", "250173424024", "250173426002", "250173421014",
      "250173421013", "250251701023", "250251701013", "250251706012"
      , "250173424004", "250251701003", "250251701006" #These three need to be added for 2017 for some reasons
      )) |>
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022)) 

## b. Figuring this out for Chelsea Y2023
Chelsea_CBG_Data_Y2023 <-
  Massachusetts_CBG_Data_Y2022[
  st_intersects(Massachusetts_CBG_Data_Y2022, Chelsea_Boundary, sparse = FALSE)[, 1], ] |>
  filter(!Census_GEOID %in% c("250173424023", "250173424024", "250173426002", "250173421014",
      "250173421013", "250251701023", "250251701013", "250251706012")) |> # Could also use %nin%
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022)) 
  
#-# Double checked both of these: they seem good above!
## c. Figuring this out for Holyoke Y2017
Holyoke_CBG_Data_Y2017 <-
  Massachusetts_CBG_Data_Y2022[
  st_intersects(Massachusetts_CBG_Data_Y2022, Holyoke_Boundary, sparse = FALSE)[, 1],
] |>
  filter(!Census_GEOID %in% c(
    "250138122021", "250138111023", "250138111022", "250138111021",
    "250138111012", "250138111024", "250138111011", "250138113013",
    "250158211012", "250158211014", "250158213001", "250158225004",
    "250158211012", "250158213003", "250158223001", "250158225004",
    "250138125004", "250158224012", "250158214004"
    , "250138122022" #This one needed to be added for 2017 for some reasons
  )) |>
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022))

## c. Figuring this out for Holyoke Y2023
Holyoke_CBG_Data_Y2023 <-
  Massachusetts_CBG_Data_Y2022[
  st_intersects(Massachusetts_CBG_Data_Y2022, Holyoke_Boundary, sparse = FALSE)[, 1],
] |>
  filter(!Census_GEOID %in% c(
    "250138122021", "250138111023", "250138111022", "250138111021",
    "250138111012", "250138111024", "250138111011", "250138113013",
    "250158211012", "250158211014", "250158213001", "250158225004",
    "250158211012", "250158213003", "250158223001", "250158225004",
    "250138125004", "250158224012", "250158214004"
  )) |>
  st_transform(st_crs(Massachusetts_CBG_Data_Y2022)) 
```

# 3. Preliminary Giveaway Data: Cleaning and making it ready for the survivorship study for Holyoke, MA and Chelsea, MA (2017 and 2024)
```{r Preliminary Data for Holyoke, MA and Chelsea, MA, echo=FALSE, message=FALSE}
# 1. Reading in all the simple spreadsheets 
## a) Reading in the Chelsea, MA Giveaway Spreadsheet
Pre_Chelsea_Holyoke_Giveaways <-
  read_csv("input_data/General/ggcp_aggregated_v6.csv") |>
  filter(city != "Leominster")

# 2. Cleaning early the datasets Chelsea, MA and Holyoke, MA before Giveaway data
Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 <-
  Pre_Chelsea_Holyoke_Giveaways |>
  transmute(
    "Unique_ID" = id           
  , "City" = city           
  # , "Random" = random         
  , "Resurvey" = resurvey       
  , "Date_Planted" = date_plant     
  , "Public_Private" = pubpriv        
  , "Address" = address        
  , "Cultivar" = cultivar       
  , "Family" = family         
  , "Genus" = genus          
  , "Species" = species        
  , "Common_Name" = comm_name      
  # , "   " = fastigiate     
  , "Replacement" = replacement    
  , "Date_Surveyed_2017" = s1_date        
  # , "Year_Surveyed_2017" = s1_year        
  , "Date_Surveyed_2025" = s2_date        
  # , "Year_Surveyed_2025" = s2_year        
  , "Land_Use_General" = s2_lu_gen      
  , "Land_Use_Specific" = s2_lu_spec     
  , "Site_Type_Specific" = s2_site        
  , "Mortality_2017" = s1_mort        
  , "Mortality_2025" = s2_mort        
  , "DBH1_2017" = s1_dbh1        
  , "DBH1_2025" = s2_dbh1        
  , "DBH1_Measured_Height_2017" = s1_mh_dbh1     
  , "DBH1_Measured_Height_2025" = s2_mh_dbh1     
  , "DBH2_2017" = s1_dbh2        
  , "DBH2_2025" = s2_dbh2        
  , "DBH2_Measured_Height_2017" = s1_mh_dbh2     
  , "DBH2_Measured_Height_2025" = s2_mh_dbh2     
  , "Height_2017" = s1_height      
  , "Height_2025" = s2_height      
  , "Width_2017" = s1_width       
  , "Width_2025" = s2_width       
  , "Vigor_2017" = s1_vigor       
  , "Vigor_2025" = s2_vigor       
  , "Condition_2017" = s1_cond        
  , "Condition_2025" = s2_cond        
  , "Sprouts_2017" = s1_sprouts     
  , "Sprouts_2025" = s2_sprouts     
  , "Notes_2017" = s1_notes       
  , "Notes_2025" = s2_notes       
  , "X" = x_coord        
  , "Y" = y_coord        
  )  |>
  st_as_sf(coords = c("X", "Y"), crs = 4326, remove = TRUE) |>
  st_transform(26986) |>
  pivot_longer(
     cols = c(ends_with("_2017"), ends_with("_2025"))
   , names_to = c(".value", "Year_Surveyed")
   , names_pattern = "(.*)_(2017|2025)$"
  ) |>
  mutate(Time_Period = case_when(
    Year_Surveyed == "2017" ~ "Time 1"
  , Year_Surveyed == "2025" ~ "Time 2"
  , TRUE ~ NA_character_
  )) |>
  mutate(Survey_Year_Dataset = Year_Surveyed)
```

# 4. iTree Data: Cleaning and preparing for Holyoke, MA and Chelsea, MA 
```{r Additional iTree Data for Holyoke, MA and Chelsea, MA, echo=FALSE, message=FALSE}
# 1. Making the Broadleaf and Conifers table
## a. Creating summary table of Broadleaf and Conifers for a join
Pre_Broadleaf_Conifer_Table_V1 <- 
  st_read("input_data/General/pre_hero_tree_giveaway_all_2025-07-15.gpkg") |> #decided just to call in the final-made one
  st_drop_geometry() |>
  filter(!is.na(Broadleaf_Conifer)) |> 
  group_by(Species, Broadleaf_Conifer) |>  
  summarize(Count = n(), .groups = "drop") |> 
  arrange(Species) |>
  tidylog::select(-Count) |>
  rename(Broadleaf_Conifer = Broadleaf_Conifer)

## b. Performing some double checks
### i) Checking for duplicates
Pre_Broadleaf_Conifer_Table_V1 |>
  count(Species) |>
  filter(n > 1) # All unique

### ii) Checking which species do not have currently a partner 
Unmatched_Leaf_Species <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  distinct(Species) |>
  anti_join(
    Pre_Broadleaf_Conifer_Table_V1 |> distinct(Species),
    by = "Species"
  ) |>
  arrange((Species))

Unmatched_Leaf_Species 

## c. Adding the additional ones
Pre_Broadleaf_Conifer_Table_V2 <- 
  tribble(
  ~Species,                      ~Broadleaf_Conifer
  , "Abies balsamea"       , "Conifer"
  , "Acer ginnala"         , "Broadleaf"
  , "Cladastis kentukea"   , "Broadleaf"
  , "Magnolia virginiana"  , "Broadleaf"
  , "Malus spp"            , "Broadleaf"
  , "Ostrya virginana"     , "Broadleaf"
  , "Oxydendron arboreum"  , "Broadleaf"
  , "Prunus (species)"     , "Broadleaf"
  , "Prunus x incisa"      , "Broadleaf"
  , "Pyrus spp"            , "Broadleaf"
  , "Ulmus spp"            , "Broadleaf"
  )

## d. Combining both tables for my join list
Broadleaf_Conifer_Table <-
 bind_rows(
    Pre_Broadleaf_Conifer_Table_V1
  , Pre_Broadleaf_Conifer_Table_V2
    ) |>
 rename_with(~ paste0("Ecological_", .x)
            , .cols = -any_of("Species"))

# 2. Creating the Native Status Table
Native_Status_Table <- 
  st_read("input_data/General/pre_hero_tree_giveaway_all_2025-07-15.gpkg") |>
  st_drop_geometry() |>
  dplyr::select(Species, Native_Status) |>
  distinct() |>
 rename_with(~ paste0("Ecological_", .x)
            , .cols = -any_of("Species"))

# 3. Creating the iTree Final Table
## a. Reading and renaming columns of the 
Pre_i_Tree_Table_V1 <- 
  read_excel(
    "input_data/General/Create_iTree_spp_codes_from_inventory_list.xls.xlsx"
    , sheet = "i-Tree Species List"
  ) |>
 mutate( # This is to make it mergeable
     Species = str_c(`Genus Name`, `Species Name`, sep = " ")
  ,  Species = str_to_lower(Species)
  ,  Species = str_replace(Species, "^\\w", toupper(str_sub(Species, 1, 1)))  # Ensures the first letter is capitalized
  ) |>
  rename(Height_Maturity = `Height at Maturity (feet)`
       , i_Tree_Code        = `Species Code`
       , Growth_Form        = `Growth Form`
       , Percent_Leaf_Type  = `Percent Leaf Type`
       , Leaf_Type          = `Leaf Type`
       , Growth_Rate        = `Growth Rate`
       ) 

## b. Performing some double checks
### i) Checking for duplicates
Pre_i_Tree_Table_V1 |>
  count(Species) |>
  filter(n > 1) # All unique

### ii) Checking which species do not have currently a partner 
Unmatched_iTree_Species <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  distinct(Species) |>
  anti_join(
    Pre_i_Tree_Table_V1 |> distinct(Species),
    by = "Species"
  )
Unmatched_iTree_Species 

## c. Creating the index table on an Excel and and reading it in
Pre_i_Tree_Table_V2 <- 
  read_excel("input_data/General/i_Tree_Appendix.xls.xlsx"
           , sheet = "i-Tree Species List"
           , n_max = 19) |>
  tidylog::select(1:8)
# For note, species like Quercus x warei, Prunus x incam, Malus x domestica, & Other [genus spp.] were the only ones without any information. This is due to not being able to find a respective i-Tree value. 

## d. Merging both these trees for potential iTree analysis
i_Tree_Table <- 
  bind_rows(
    Pre_i_Tree_Table_V1
  , Pre_i_Tree_Table_V2
    ) |>
  tidylog::select(-`Genus Name`, -`Species Name`, -Synonym, -Family, -Order, -Class, -`Common Name`) |>
  rename_with(~ paste0("Ecological_", .x)
          , .cols = -any_of(c("Genus_Name","Species","i_Tree_Code")) # Trying to make these not be note selected
    )

```

# 5. Tree Giveaway Data: Merging all datasets and final cleanings for Holyoke, MA and Chelsea, MA (2017 and 2023)
```{r Final Tree Giveawy for Holyoke, MA and Chelsea, MA}
# 1. Merging all of the datasets into one | Property and Census specifically
## a) Preparing the necessary layers needed
Pre_Parcel_Data_Y2019 <- 
  bind_rows(Holyoke_Property_Data_Y2019, Chelsea_Property_Data_Y2019) |>
  tidylog::select(-Time_Period) |>
  filter(!is.na(Parcel_Unique_ID))

Pre_Parcel_Data_Y2023 <- 
  bind_rows(Holyoke_Property_Data_Y2023, Chelsea_Property_Data_Y2023) |>
  tidylog::select(-Time_Period) |>
  filter(!is.na(Parcel_Unique_ID))

Pre_Census_Data_Y2017 <- 
  bind_rows(Holyoke_CBG_Data_Y2017, Chelsea_CBG_Data_Y2017) 

Pre_Census_Data_Y2023 <- 
  bind_rows(Holyoke_CBG_Data_Y2023, Chelsea_CBG_Data_Y2023) 

## b. Calculating each giveaway and choosing the largest polygon
### i) Finding for parcels, which is more challenging since not all are within
#### 2019, Finding all that joined
Pre_Giveaways_Parcel_Data_Y2019 <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  filter(Time_Period == "Time 1") |>
  st_join(Pre_Parcel_Data_Y2019, left = TRUE, largest = TRUE)

#### 2019, Finding all within
Within_Giveaways_Parcel_Data_Y2019 <-
  Pre_Giveaways_Parcel_Data_Y2019 |>
  filter(!is.na(Parcel_Property_ID)) |>
  st_drop_geometry() |>
  tidylog::select(City, Time_Period, Unique_ID, starts_with("Parcel"))

#### 2019, Finding all missing points
Missing_Giveaways_Parcel_Data_Y2019 <- 
  Pre_Giveaways_Parcel_Data_Y2019 |>
  filter(is.na(Parcel_Property_ID))

#### 2019, Finding nearest parcel for missing points
Nearest_Indexes_Y2019 <- 
  Missing_Giveaways_Parcel_Data_Y2019 |>
  st_nearest_feature(Pre_Parcel_Data_Y2019)

#### 2019, Combine IDs with nearest parcel attributes
Nearest_Giveaways_Parcel_Data_Y2019 <- 
  Pre_Parcel_Data_Y2019 |> 
  st_drop_geometry() |>                                      
  slice(Nearest_Indexes_Y2019) |> # Selecting based on the closest parcels
  tidylog::select(starts_with("Parcel")) |>                   
  bind_cols(
    Missing_Giveaways_Parcel_Data_Y2019 |> 
      st_drop_geometry() |> 
      tidylog::select(City, Time_Period, Unique_ID) 
  )

#### 2019, Merging the Dataset
Giveaway_Parcel_Data_Y2019 <- 
  bind_rows(Within_Giveaways_Parcel_Data_Y2019
          , Nearest_Giveaways_Parcel_Data_Y2019)

#### 2023, Finding all that joined
Pre_Giveaways_Parcel_Data_Y2023 <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  filter(Time_Period == "Time 2") |>
  st_join(Pre_Parcel_Data_Y2023, left = TRUE, largest = TRUE)

#### 2023, Finding all within
Within_Giveaways_Parcel_Data_Y2023 <-
  Pre_Giveaways_Parcel_Data_Y2023 |>
  filter(!is.na(Parcel_Property_ID)) |>
  st_drop_geometry() |>
  tidylog::select(City, Time_Period, Unique_ID, starts_with("Parcel"))

#### 2023, Finding all missing points
Missing_Giveaways_Parcel_Data_Y2023 <- 
  Pre_Giveaways_Parcel_Data_Y2023 |>
  filter(is.na(Parcel_Property_ID))

#### 2023, Finding nearest parcel for missing points
Nearest_Indexes_Y2023 <- 
  Missing_Giveaways_Parcel_Data_Y2023 |>
  st_nearest_feature(Pre_Parcel_Data_Y2023)

#### 2023, Combine IDs with nearest parcel attributes
Nearest_Giveaways_Parcel_Data_Y2023 <- 
  Pre_Parcel_Data_Y2023 |> 
  st_drop_geometry() |>                                      
  slice(Nearest_Indexes_Y2023) |> # Selecting based on the closest parcels
  tidylog::select(starts_with("Parcel")) |>                   
  bind_cols(
    Missing_Giveaways_Parcel_Data_Y2023 |> 
      st_drop_geometry() |> 
      tidylog::select(City, Time_Period, Unique_ID) 
  )

#### 2023, Merging the Dataset
Giveaway_Parcel_Data_Y2023 <- 
  bind_rows(Within_Giveaways_Parcel_Data_Y2023
          , Nearest_Giveaways_Parcel_Data_Y2023)

### i) Finding for census
Giveaway_Census_Data_Y2017 <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |> 
  mutate(Temp_Row_ID = row_number()) |>
  filter(Time_Period == "Time 1") |> 
  st_join(Pre_Census_Data_Y2017, left = TRUE, largest = TRUE) |> # Selecting the largest
  st_drop_geometry() |>
  tidylog::select(City, Time_Period, Unique_ID, starts_with("Census"))

Giveaway_Census_Data_Y2023 <- 
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |> 
  mutate(Temp_Row_ID = row_number()) |>
  filter(Time_Period == "Time 2") |> 
  st_join(Pre_Census_Data_Y2023, left = TRUE, largest = TRUE) |> # Selecting the largest
  st_drop_geometry() |>
  tidylog::select(City, Time_Period, Unique_ID, starts_with("Census"))

## c. Merging the columns based on theme
Giveaway_Parcel_Data_Y2019_Y2023 <-
  bind_rows(Giveaway_Parcel_Data_Y2019, Giveaway_Parcel_Data_Y2023)

Giveaway_Census_Data_Y2017_Y2024 <-
  bind_rows(Giveaway_Census_Data_Y2017, Giveaway_Census_Data_Y2023)

## d. Combining all of the them together and some significant cleaning
Pre_V2_Giveaway_Y2017_Y2024 <-
  Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  left_join(Giveaway_Parcel_Data_Y2019_Y2023, by = c("City", "Time_Period", "Unique_ID")) |>
  left_join(Giveaway_Census_Data_Y2017_Y2024, by = c("City", "Time_Period", "Unique_ID")) 

# 2. Cleaning the final dataset for its final preparation
Pre_V3_Giveaway_Y2017_Y2024 <-
  Pre_V2_Giveaway_Y2017_Y2024 |>
  mutate(Address = str_to_upper(Address)
       , Public_Private = str_to_title(Public_Private)) |> #Making all the addresses capitals
  arrange(City, Address) |>
  mutate(
    Date_Surveyed = ymd(Date_Surveyed)
  , Date_Planted = mdy(Date_Planted)
  , Year_Planted = year(Date_Planted)
  , Month_Planted = month(Date_Planted)
  , Season_Planted = case_when(
      Month_Planted %in% c(3, 4, 5, 6, 7) ~ "SPRING" #EM: Still need to check with John Rogan 
    , Month_Planted %in% c(8, 9, 10, 11) ~ "FALL" 
    , TRUE ~ NA_character_      
    )
  ) |>
  mutate(
         Vigor = as.character(Vigor)
       , Vigor = case_when(
            Vigor == "1" ~ "100% to 90% Full"
          , Vigor == "2" ~ "90% to 75% Full"
          , Vigor == "3" ~ "75% to 50% Full"
          , Vigor %in% c("4","5") ~ "50% or less" #EM: 5—Seems to be only in Y2017 (~70) and seems to be a further subset of bad conditions so will group there. Should double check though
          , TRUE ~ NA_character_) #approx. 6 other counts of other numbers from 2017 & 0 serves the same purpose s NA_character_ so making everything else NA
        , Site_Type_General = case_when(
            Site_Type_Specific %in% c("Sidewalk Cutout", "Sidewalk Planting Strip") ~ "Street Trees"
          , Site_Type_Specific %in% c("Backyard", "Front Yard", "Side Yard") ~ "Yard Trees"
          , TRUE ~ "Maintained Area Trees")) |>
    left_join(Broadleaf_Conifer_Table, by = "Species") |>
    left_join(i_Tree_Table, by = "Species") |>
    left_join(Native_Status_Table, by = "Species") |>
    distinct(City, Unique_ID, Date_Planted, Time_Period, .keep_all = TRUE) #This is just to remove a Duplicate of C2409; the only one which has the same values, too

# 3. Calculating Days in Ground for All Data
Giveaway_Survival_Data <- 
  Pre_V3_Giveaway_Y2017_Y2024 |>
   # filter(Site_Type_Specific == c("Backyard", "Front Yard")) |>
  st_drop_geometry() |>
  tidylog::select(City, Unique_ID, Date_Planted, Time_Period, Date_Surveyed, Mortality) |>
  pivot_wider(
    id_cols     = c(City, Unique_ID, Date_Planted)
  , names_from  = Time_Period          
  , values_from = c(Date_Surveyed, Mortality)
  , names_sep   = "_"
  ) |>
  rename(
     Date_Surveyed_Time_1     = `Date_Surveyed_Time 1`  
   , Date_Surveyed_Time_2     = `Date_Surveyed_Time 2`   
   , Mortality_Time_1  = `Mortality_Time 1`
   , Mortality_Time_2  = `Mortality_Time 2`
  ) |>
  filter(!(is.na(Mortality_Time_1) & is.na(Mortality_Time_2)))|> #The issue is this;for some reason, there are present survey dates in the dataset for 2024 that have had no Mortality taken in either T1 or T2. Because my analysis does not flag NA_, only death events and assumes everything is living otherwise, this is causing significant issues later down in the road when I subset the dataset to only look for T1 Dead | T1 Alive, T2 Dead | T2 Alive
  # Flag for when to stop counting for the days
  mutate(
    Dead_Time_1 = Mortality_Time_1 %in% c("Standing Dead", "Removed", "Stump")
  , Dead_Time_2 = Mortality_Time_2 %in% c("Standing Dead", "Removed", "Stump")
  # Selecting the day to stop counting
  , Inspection_Date = case_when(
      Dead_Time_1 ~ Date_Surveyed_Time_1 #Dead in 2017, Stop 2017
    , !Dead_Time_1 & Dead_Time_2 ~ Date_Surveyed_Time_2 #Alive in 2017, Dead in 2024, Stop 2024
    , TRUE ~ coalesce(Date_Surveyed_Time_2, Date_Surveyed_Time_1)
    )) |>
    filter(!is.na(Inspection_Date)) |>
  mutate(
  , Days_In_Ground = as.integer(Inspection_Date - Date_Planted) # Calculating based off of the rules set in inspection date
  # The following is for the survival analysis
  , Status = case_when(  
      Dead_Time_1 ~ 1
    , !Dead_Time_1 & Dead_Time_2 ~ 1
    , TRUE ~ 0
    )
  # Tells us when the date stopped
  , Inspection = case_when(
       Dead_Time_1 ~ 1
     , !Dead_Time_1 & Dead_Time_2 ~ 2
     , TRUE ~ if_else(!is.na(Date_Surveyed_Time_2), 2, 1)
    )
  ) |>
  mutate( # Need this for later analysis
    Has_T1_Observation = !is.na(Mortality_Time_1) # has mortality status presence
  , Has_T2_Observation = !is.na(Mortality_Time_2)
  ) |>
  filter(!is.na(Days_In_Ground), Days_In_Ground >= 0) |>
  filter(Inspection_Date < ymd("2025-07-01")) |>
  tidylog::select(City, Unique_ID, Days_In_Ground, Status, Inspection, Inspection_Date, Has_T1_Observation, Has_T2_Observation) |>
  rename_with(
        ~ paste0("Survival_", .x)
      , .cols = !any_of(c("City", "Unique_ID"))
    )

# 4. Creating the final dataset for its final preparation
## a) Finding the median of the survey dates of 2017 to superimpose them on trees with only a survey 2024 date that was planted before and survived
Median_Survey_Date_Y2017 <-
  Pre_V3_Giveaway_Y2017_Y2024 |>
  st_drop_geometry() |>
  filter(Year_Surveyed == "2017") |>
  summarise(Median_Survey_Date = median(Date_Surveyed, na.rm = TRUE)) |>
  pull(Median_Survey_Date)

## c) Putting together the final spreadsheet
Final_Giveaway_Y2017_Y2024 <-
  Pre_V3_Giveaway_Y2017_Y2024 |>
  left_join(Giveaway_Survival_Data, by = c("City","Unique_ID")) |>
  #--# Changes made to make sure we are having differences in the values
  mutate(
      Date_Surveyed = case_when(
        Time_Period == "Time 1" & !is.na(Date_Planted) & Date_Planted > lubridate::as_date("2017-06-27") ~ NA_Date_
      , Time_Period == "Time 1" & Survival_Status == 0 & Survival_Inspection == 2 & Date_Planted < lubridate::as_date("2017-05-30") ~ lubridate::as_date(Median_Survey_Date_Y2017)
      , TRUE ~ Date_Surveyed
    )) |>
  mutate(
    Survival_Days_In_Ground = case_when(
       Time_Period == "Time 2" & Survival_Status == 1 & Survival_Inspection == 1 ~ NA_integer_
     , Time_Period == "Time 1" & Date_Planted > Date_Surveyed ~ NA_integer_
     , Time_Period == "Time 1" & Date_Planted > lubridate::as_date("2017-05-30") ~ NA_integer_ #EM: Please double check to ensure that you get all the right ones, here
     , !is.na(Date_Surveyed) & !is.na(Date_Planted) ~ as.integer(Date_Surveyed - Date_Planted)
     , TRUE ~ Survival_Days_In_Ground
    )
  )|>
  #---#
  # Creating the qualitative group for each quantitative for analysis (Kaplan-Meier), using tertiles for statistical stability
  mutate(
 # Parcels
   Parcel_Total_Value_Group = ntile(Parcel_Total_Value, 3)
 , Parcel_Total_Value_Group = factor(Parcel_Total_Value_Group, labels = c("Low", "Medium", "High")),
 , Parcel_Lot_Size_Group = ntile(Parcel_Lot_Size, 3)
 , Parcel_Lot_Size_Group = factor(Parcel_Lot_Size_Group, labels = c("Small", "Medium",  "Large")),
 , Parcel_Age_Group = ntile(Parcel_Year_Built, 3)
 , Parcel_Age_Group = factor(Parcel_Age_Group, labels = c("Oldest", "Middle", "Newest"))
 # Census
 , Census_Income_Group = ntile(Census_Median_Household_Income, 3)
 , Census_Income_Group = factor(Census_Income_Group,labels = c("Low Income", "Middle Income", "High Income"))
 , Census_HS_Education_Group = ntile(Census_Percentage_At_Least_High_School, 3)
 , Census_HS_Education_Group = factor(Census_HS_Education_Group, labels = c("Low HS", "Medium HS", "High HS"))
 , Census_College_Education_Group = ntile(Census_Percentage_At_Least_College, 3)
 , Census_College_Education_Group = factor(Census_College_Education_Group, labels = c("Low College", "Medium College", "High College"))
 , Census_White_Group = ntile(Census_White_Alone_Percentage, 3)
 , Census_White_Group = factor(Census_White_Group, labels = c("Low White", "Medium White", "High White"))
 , Census_Unemployment_Group = ntile(Census_Unemployed_Percentage, 3)
 , Census_Unemployment_Group = factor(Census_Unemployment_Group, labels = c("Low Unemployment", "Medium", "High Unemployment"))
 , Census_Poverty_Group = ntile(Census_Below_Poverty_Percentage, 3)
 , Census_Poverty_Group = factor(Census_Poverty_Group, labels = c("Low Poverty", "Medium", "High Poverty"))
 , Census_Ownership_Group = ntile(Census_Owner_Occupied_Percentage, 3)
 , Census_Ownership_Group = factor(Census_Ownership_Group, labels = c("Low Ownership", "Medium", "High Ownership"))
) |>
  #---#
  tidylog::select(
   Unique_ID                              
 , City                                   
 , Address                                
 , Date_Planted                           
 , Year_Planted                           
 , Month_Planted                                 
 , Season_Planted                         
 , Year_Surveyed                          
 , Time_Period                            
 , Replacement                            
 , Resurvey                               
 , Species                                
 , Genus                                  
 , Family                                 
 , Cultivar                               
 , Common_Name                            
 , Land_Use_General                       
 , Land_Use_Specific                      
 , Site_Type_General
 , Site_Type_Specific                             
 , Public_Private                         
 , Date_Surveyed                          
 , Mortality                              
 , Vigor                                  
 , Sprouts                                
 , Condition                              
 , Height                                 
 , Width                                  
 , DBH1                                   
 , DBH1_Measured_Height                   
 , DBH2
 , DBH2_Measured_Height
 , Notes                                  
 , i_Tree_Code    
 , Survey_Year_Dataset
 , Parcel_Year_Dataset  
 , Ecological_Broadleaf_Conifer   
 , Ecological_Native_Status
 , Ecological_Growth_Form                 
 , Ecological_Percent_Leaf_Type           
 , Ecological_Leaf_Type                   
 , Ecological_Growth_Rate                 
 , Ecological_Longevity                   
 , Ecological_Height_Maturity             
 , Parcel_Location_ID                     
 , Parcel_Unique_ID                       
 , Parcel_Property_ID                     
 , Parcel_Lot_Size 
 , Parcel_Lot_Size_Group
 , Parcel_Lot_Date                        
 , Parcel_Lot_Units                       
 , Parcel_Lot_Price                       
 , Parcel_Building_Value                  
 , Parcel_Land_Value                      
 , Parcel_Other_Value                     
 , Parcel_Total_Value  
 , Parcel_Total_Value_Group
 , Parcel_Year                            
 , Parcel_Style                           
 , Parcel_Use_Code                        
 , Parcel_Zoning                          
 , Parcel_Year_Built 
 , Parcel_Age_Group
 , Parcel_Building_Area                   
 , Parcel_Residential_Area                
 , Parcel_Units                           
 , Parcel_Number_Rooms                    
 , Parcel_Stories     
 , Census_GEOID                           
 , Census_Block                           
 , Census_Tract                           
 , Census_County                          
 , Census_State                           
 , Census_Median_Household_Income         
 , Census_Total_Population                
 , Census_Households                      
 , Census_Ethnicity_Population            
 , Census_White_Alone                     
 , Census_Black_Alone                     
 , Census_American_Indian_Alone           
 , Census_Asian_Alone                     
 , Census_Native_Hawaiian_Alone           
 , Census_Other_Alone                     
 , Census_Two_or_More_Alone               
 , Census_Hispanic_Alone                  
 , Census_Poverty_Population              
 , Census_Below_Poverty                   
 , Census_Labor_Population                
 , Census_Unemployed                      
 , Census_Median_Age                      
 , Census_Total_Population_Over_25        
 , Census_Median_Gross_Rent               
 , Census_Median_Home_Value               
 , Census_Tenure_Population               
 , Census_Owner_Occupied                  
 , Census_Renter_Occupied                 
 , Census_Percentage_At_Least_High_School 
 , Census_Percentage_At_Least_College     
 , Census_White_Alone_Percentage          
 , Census_Black_Alone_Percentage          
 , Census_American_Indian_Alone_Percentage
 , Census_Asian_Alone_Percentage          
 , Census_Native_Hawaiian_Alone_Percentage
 , Census_Other_Alone_Percentage          
 , Census_Two_or_More_Alone_Percentage    
 , Census_Hispanic_Alone_Percentage       
 , Census_Below_Poverty_Percentage        
 , Census_Unemployed_Percentage           
 , Census_Owner_Occupied_Percentage       
 , Census_Renter_Occupied_Percentage    
 , Census_Income_Group 
 , Census_HS_Education_Group 
 , Census_College_Education_Group
 , Census_White_Group 
 , Census_Unemployment_Group 
 , Census_Poverty_Group
 , Census_Ownership_Group
 , Survival_Days_In_Ground                 
 , Survival_Status                         
 , Survival_Inspection                     
 , Survival_Inspection_Date                
 , Survival_Has_T1_Observation             
 , Survival_Has_T2_Observation             
 , geometry                               
  ) |>
  rename(Census_GeoID = Census_GEOID) |>
  mutate(
    Year_Planted = as.integer(Year_Planted)
  , Year_Surveyed = as.integer(Year_Surveyed)
  , Parcel_Year = as.integer(Parcel_Year)
  , Survey_Year_Dataset = as.integer(Survey_Year_Dataset)
  , Parcel_Year_Dataset  = as.integer(Parcel_Year_Dataset)
  ) |>
  mutate(
  Parcel_Style = case_when(
     Parcel_Style %in% c("2 Family", "2 FAMILY", "Duplex", "DUPLEX") ~ "2 Family/Duplex"
   , Parcel_Style %in% c("3 Family", "3 FAMILY") ~ "3 Family"
   , Parcel_Style %in% c("4-8 FAM", "Apt 4", "Apt 5-8", "APT-9+", "APRTMNT-GN", "APRTMNT-HR") ~ "Multi-Family (4+)"
   , Parcel_Style %in% c("Bungalow") ~ "Bungalow"
   , Parcel_Style %in% c("Cape Cod", "CAPE") ~ "Cape Cod"
   , Parcel_Style %in% c("Colonial", "COLONIAL") ~ "Colonial"
   , Parcel_Style %in% c("Commercial") ~ "Commercial"
   , Parcel_Style %in% c("Condo-Twnhse", "Condominium") ~ "Condominium/Townhouse"
   , Parcel_Style %in% c("Conventional") ~ "Conventional"
   , Parcel_Style %in% c("Raised Ranch", "Ranch", "RANCH") ~ "Ranch"
   , Parcel_Style %in% c("ROW HOUSE", "Row House 108") ~ "Row House"
   , Parcel_Style %in% c("Outbuildings") ~ "Outbuilding"
   , Parcel_Style %in% c("Vacant Land") ~ "Vacant Land"
   , Parcel_Style %in% c("OLD STYLE") ~ "Old Style"
   , TRUE ~ "Other/Unclassified"
  )) # Needed to standardize the names

Final_Giveaway_Y2017_Y2024 |>
st_write(paste0("input_data/Final_Giveaway_Y2017_Y2024_", Sys.Date(), ".gpkg")) # Changed it to have the sys.date
```

# 6. Double Checks: Ensuring the datasets merged correctly for Holyoke, MA and Chelsea, MA (2017 and 2023) 
## a) Double checking correct dates and other issues
```{r}
# 1. Number of Rows Per Trees | Should be 2
Final_Giveaway_Y2017_Y2024 |>
  st_drop_geometry() |>
  count(Unique_ID, name = "n_rows") |>
  tabyl(n_rows)
# ESM: All rows have 2, which is good!

# 2. Negative Days | An issue seen earlier
Final_Giveaway_Y2017_Y2024 |>
  st_drop_geometry() |>
  summarise(
    Count = n()
  , `Negative Days` = sum(Survival_Days_In_Ground < 0, na.rm = TRUE)
  )
# ESM: All good, no negative survival days, so that was fixed

# 3. Correct Time Days Length | Since T1 and T2 differ, making sure that T2 > T1 or T2 = T1 (those are for the ones that currently have NA and Alive in T1), unless its NA since T1 = Dead 
DC_Vector_Dead_T1 <-
  Final_Giveaway_Y2017_Y2024 |>
  filter(Time_Period == "Time 1", Survival_Status == 1, Survival_Inspection == 1) |>
  distinct(Unique_ID) |>
  pull(Unique_ID) # Pull is kinda like the unique($), but you can |> it through just for your vectoe

DC_Time_Day_T1 <-
  Final_Giveaway_Y2017_Y2024 |> 
  st_drop_geometry() |>
  tidylog::select(Unique_ID, Time_Period, Survival_Days_In_Ground, Survival_Status, Survival_Inspection) |>
  tidyr::pivot_wider(
    names_from  = Time_Period
  , values_from = Survival_Days_In_Ground
  ) |>
  dplyr::mutate(
    Died_at_T1 = Unique_ID %in% DC_Vector_Dead_T1, # Making sure to not include those
    Compliance = (Died_at_T1 & is.na(`Time 2`)) | (!is.na(`Time 1`) & !is.na(`Time 2`) & `Time 2` >= `Time 1`) | (is.na(`Time 1`) & is.na(`Time 2`)) | (is.na(`Time 1`) & !is.na(`Time 2`)) #These are all acceptable conditions as listed in the earlier description
  )

DC_Time_Day_T1 |>
  summarise(`Proportion Compliance` = mean(Compliance, na.rm = TRUE)) 
#ESM: This good, all of them are complying :)

# 4. T1 Died Must Have T2 NAs
Final_Giveaway_Y2017_Y2024 |>
  st_drop_geometry() |>
  filter(Time_Period == "Time 2", Survival_Status == 1, Survival_Inspection == 1) |>
  summarise(All_T2_NAs = all(is.na(Survival_Days_In_Ground)))
# ESM: True, so they are all NAs!

# 5. Equal Days in T1 & T2 | There should not be any
Final_Giveaway_Y2017_Y2024 |>
  st_drop_geometry() |>
  tidylog::select(Unique_ID, Time_Period, Survival_Days_In_Ground) |>
  pivot_wider(
    names_from  = Time_Period
  , values_from = Survival_Days_In_Ground
  ) |>
  filter(!is.na(`Time 1`), !is.na(`Time 2`), `Time 1` == `Time 2`) |>
  view() 
# ESM to DHL: This is a problem but I reviewed the issue. Basically, these trees are the ones that were never surveyed in T1 and then they were reported as dead in T2. Either one of two things happened—(A) Alive T1, Dead T2 (B) Dead T1, Dead T2 (surveyed twice). On the more usual case, they were dead in T1; however, we cannot say that because they are NAs. We have to wait for John Rogan's response to see how to treat those cases. Our options, however, if not is either (A) overestimate mortality (B) underestimate mortality (C) exclude all together, limiting statistical power. Thoughts?

# 6. Planting must happen before the survey date!
Final_Giveaway_Y2017_Y2024 |>
  filter(!is.na(Date_Planted), !is.na(Date_Surveyed), Date_Surveyed < Date_Planted) |>
  tidylog::select(Unique_ID, City, Time_Period, Date_Planted, Date_Surveyed, Survival_Days_In_Ground) |>
  arrange(Date_Surveyed) 
# ESM: Doesn't seem to be any! :) That means the code earlier worked


# 7. Listed as Dead in T1 | Alive in T2 (Checking if any exists)
Dead <- c("Standing Dead","Removed","Stump") # Setting the values that are dead, like our mortality study

Final_Giveaway_Y2017_Y2024 |> 
  mutate(Dead_Status = Mortality %in% Dead) |> #Give me TRUES for those that are dead
  filter(Survival_Status == 1 & !Dead_Status) |> #Tell me which ones we just calculated as ALIVE but are DEAD (based on the calc earlier before the join)
  filter(Time_Period == "Time 2") |> 
  tidylog::select(Unique_ID, Time_Period, Mortality, Survival_Status, Survival_Inspection, Date_Planted, Date_Surveyed, Survival_Days_In_Ground) |>
  view()

#ESM to DHL: These seem to be replantings. Basically, they are sharing the same IDs as the dead trees; however, they are currently alive with the new survey but have no Days_In_Ground because they were considered Dead in T1. There are two things we can do (1) Create new IDs for them and join them to our dataset; (2) Keep them exlcuded from our dataset. Not sure if we want to exclude replantings all together, include them as separate trees. Thoughts?

# 8. Listed as Blank in T1 | Dead in T2 (Checking how many are there)
Final_Giveaway_Y2017_Y2024 |> 
  mutate(Dead_Status = Mortality %in% Dead) |>
  filter(Survival_Status == 1 & !Dead_Status) |>
  filter(Time_Period == "Time 1") |>
  filter(is.na(Mortality)) |> # Doesn't have a mortality, so blank
  tidylog::select(Unique_ID, Time_Period, Mortality, Survival_Status,
         Survival_Inspection, Date_Planted, Date_Surveyed, Survival_Days_In_Ground) |>
  filter(!is.na(Survival_Days_In_Ground)) |>
  view() # Sort of the opposite, we are checking when they are considered DEAD but are calculated as Alive (based on the cal earlier before the join) | it is not that they are alive, but

# ESM to DHL: As mentioned earlier, these are the trees that seem to have blanks and are considered dead in T2 (making sure we also do not include those that have had a previous value saying they are dead). The issue here is that these are making the survival rate really high, when in reality, like mentioned earlier—they likely could've died in T2. A question of what we should do moving forward now. 
```

## b) Double checking missignness with mortality
```{r}
# 1. Figuring out total missing mortality 
Final_Giveaway_Y2017_Y2024 |> 
  mutate(Mortality = case_when(
    Mortality %in% c("Removed","Standing Dead","Stump") ~ "Dead"
  , Mortality == "Alive" ~ "Alive"
  , TRUE ~ Mortality ) )|> #John: Confirm why there would be an NA; they got the planting records from the city and went to a random sub-set; did not have the human-power to go to all of them. They have the lat-long, and the species, and the date, and they weren't able to add their own data from HERO. 
  st_drop_geometry() |>
  tabyl(City, Time_Period, Mortality) # This is the issue <- there are SO many NAs in 2017; checked the original data, and it has the same finding. Thus, survivorship becomes challenging to determine and gives the false sense of high survivorship rate. That is the issue currently with the data. But still, pretty interesting

#ESM to DHL: Although not perfect, we basically have the entire dataset; there is some missingness, but I think we are fine to move on, now :)

# Chelsea
## 2017: 385 vs 432 (Present vs Presentation)
## 2025: 1508 vs 1509 (Present vs Presentation)

# Holyoke
## 2017: 730 vs 842 (Present vs Presentation)
## 2025: 1500 vs 1500 (Present vs Presentation)

# 2. Figuring out when they were surveyed in both
# Total Resurveyed
Final_Giveaway_Y2017_Y2024 |>
  filter(Resurvey == 1) |>
  count(City, name = "Count_Resurvey_1")

# Chelsea: 385 vs 385 (Present vs Presentation)
# Holyoke: 735 vs 731 (Present vs Presentation)

# Total Based on Public_Private
Final_Giveaway_Y2017_Y2024 %>%
  filter(Resurvey == 1) %>%
  count(City, Public_Private, name = "Count_Resurvey_1")
# Chelsea
## Public: 349 vs 325 (Present vs Presentation)
## Private: 39 vs 60 (Present vs Presentation)

# Holyoke
## Public: 522 vs 417 (Present vs Presentaiton)
## Private: 213 vs 341 (Present vs Presentation)

```

## c) Checking Other
```{r Double Checks for Holyoke, MA and Chelsea, MA, eval=FALSE, include=FALSE}
# 1. Missingness
Final_Giveaway_Y2017_Y2024 |> 
 map(~sum(is.na(.))) |>
 bind_rows() |>
 t()

# 2. Categorical Variables
Final_Giveaway_Y2017_Y2024 |> 
  st_drop_geometry() |>
  tidylog::select(-Address) |> 
  mutate_if(is.character, as.factor) |>
  select_if(is.factor) |>
  map(~tabyl(.)) |>
  bind_rows(.id = 'var') # Some Survival missing but nothing alarming—perhaps the day of those in the days-in-ground but I think I made it  

# 3. Years Planted
Final_Giveaway_Y2017_Y2024 |> 
  mapview(zcol = 'Year_Planted')

# 4. Site Types
Final_Giveaway_Y2017_Y2024 |> 
  filter(Time_Period == "Time 2") |>
  st_drop_geometry() |> 
  tabyl(Site_Type_Specific)

# 5. Site Types | Graph
Final_Giveaway_Y2017_Y2024 |> 
  st_drop_geometry() |> 
  group_by(Site_Type_Specific, City) |> 
  count() |> 
  ggplot(aes(n, Site_Type_Specific)) +
  geom_col() +
  facet_wrap(~City) +
  theme_bw(16) +
  NULL

Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  filter(Time_Period == "Time 2") |>
  st_drop_geometry() |> 
  tabyl(Site_Type_Specific)

Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 |>
  filter(Time_Period == "Time 1") |>
  st_drop_geometry() |> 
  tabyl(Site_Type_Specific)

temp_Pre_Chelsea_Holyoke_Giveaways_Y2017_Y2024 <-
  bind_rows(Holyoke_Giveaway_Data_Y2017_Y2024, Chelsea_Giveaway_Data_Y2017_Y2024) |>
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326)
```


# 7. Archived: Census and Address Code for Geopackages 
```{r echo=TRUE, Archived Data Wrangling eval=FALSE}
#------INSERT IN CHAPTER 2. CENSUS DATA | STEP 0 ------# 
# 1. Loading the list of variables from ACS of 2022
## b. Listing the 2022 variable data
Variables_ACS_Y2022 <-
  tidycensus::load_variables('acs5', year = 2022) |>
  filter(geography == "block group")

# 2. Selecting the variables of ACS of 2022
## a. Selecting the 2022 variables
My_Vars <-
  c('Median_Household_Income'               = 'B19013_001'
  , 'Total_Population'                      = 'B01003_001'
  , 'Households'                            = 'B11001_001'
  , 'Ethnicity_Population'                  = 'B03002_001'
  , 'White_Alone'                           = 'B03002_003'
  , 'Black_Alone'                           = 'B03002_004'
  , 'American_Indian_Alone'                 = 'B03002_005'
  , 'Asian_Alone'                           = 'B03002_006'
  , 'Native_Hawaiian_Alone'                 = 'B03002_007'
  , 'Other_Alone'                           = 'B03002_008'
  , 'Two_or_More_Alone'                     = 'B03002_009'
  , 'Hispanic_Alone'                        = 'B03002_012'
  , 'Poverty_Population'                    = 'B17010_001'
  , 'Below_Poverty'                         = 'B17010_002'
  , 'Labor_Population'                      = 'B23025_003' # EM: Might need double-checking
  , 'Unemployed'                            = 'B23025_005'
  , 'Median_Age'                            = 'B01002_001'
  , 'Total_Population_Over_25'              = 'B15003_001'
  , 'ed_High_School_Degree'                 = 'B15003_017'
  , 'ed_GED_Degree'                         = 'B15003_018'
  , 'ed_College_Less_1_Year'                = 'B15003_019'
  , 'ed_College_More_1_Year'                = 'B15003_020'
  , 'ed_Associate_Degree'                   = 'B15003_021'
  , 'ed_plus_Bachelor_Degree'               = 'B15003_022'
  , 'ed_plus_Master_Degree'                 = 'B15003_023'
  , 'ed_plus_Professional_Degree'           = 'B15003_024'
  , 'ed_plus_Doctorate_Degree'              = 'B15003_025'
  , 'Median_Gross_Rent'                     = 'B25064_001'
  , 'Median_Home_Value'                     = 'B25077_001' 
  , 'Tenure_Population'                     = 'B25003_001'
  , 'Owner_Occupied'                        = 'B25003_002'
  , 'Renter_Occupied'                       = 'B25003_003'
      )

# 3. Selecting the variables of ACS of 2017 & 2023
## b. Downloading the Y2022 Census Data
Massachusetts_CBG_Data_Y2022 <-
  tidycensus::get_acs(
      geography = 'block group'
    , state = 'Massachusetts'
    , variables = My_Vars           
    , year = 2022
    , geometry = TRUE
    , output = 'wide'
    , moe_level = 95
    ) |>
    tidylog::select(-dplyr::ends_with('M')) |>
    tidylog::rename_with(~ gsub('E$', '', .x), dplyr::ends_with('E')) |>
    rename(NAME = NAM) |> 
    tidyr::separate( 
        NAME
      , into = c('Block', 'Tract', 'County', 'State')
      , sep  = '; ' #
      ) |> 
    tidylog::mutate(
      Total_Population_Over_25 = dplyr::na_if(Total_Population_Over_25, 0)
    , Percentage_At_Least_High_School =
        rowSums(dplyr::across(dplyr::starts_with('ed_')), na.rm = TRUE) /
        Total_Population_Over_25
    , Percentage_At_Least_College =
        rowSums(dplyr::across(dplyr::starts_with('ed_plus')), na.rm = TRUE) /
        Total_Population_Over_25
    ) |>
    mutate(White_Alone_Percentage = (White_Alone/Ethnicity_Population)
         , Black_Alone_Percentage = (Black_Alone/Ethnicity_Population)
         , American_Indian_Alone_Percentage = (American_Indian_Alone/Ethnicity_Population)
         , Asian_Alone_Percentage = (Asian_Alone/Ethnicity_Population)
         , Native_Hawaiian_Alone_Percentage = (Native_Hawaiian_Alone/Ethnicity_Population)
         , Other_Alone_Percentage = (Other_Alone/Ethnicity_Population)
         , Two_or_More_Alone_Percentage = (Two_or_More_Alone/Ethnicity_Population)
         , Hispanic_Alone_Percentage = (Hispanic_Alone/Ethnicity_Population)
         , Below_Poverty_Percentage = (Below_Poverty/Poverty_Population)
         , Unemployed_Percentage = (Unemployed/Labor_Population)
         , Owner_Occupied_Percentage = (Owner_Occupied/Tenure_Population)
         , Renter_Occupied_Percentage = (Renter_Occupied/Tenure_Population)
         ) |>
    tidylog::select(-dplyr::starts_with('ed_')) |>
    rename_with(~ paste0("Census_", .x)
              ,  .cols = -any_of("geometry")) |>
    st_transform(crs = 26986)

### i) Writing this out 
Massachusetts_CBG_Data_Y2022 |>
  st_write("input_data/General/Massachusetts_CBG_Data_Y2022.gpkg")

#------INSERT IN CHAPTER 3. ADDRESS DATA | STEP 0 ------# 
### iv) Finding all the Addresses that Need Appending
Holyoke_Giveaway_Addresses <-
  Pre_Holyoke_Giveaway_Data_Y2017_Y2024 |>
  reverse_geocode(
    lat = Latitude
  , long = Longitude
  , method = "arcgis"
  , full_results = TRUE
  )

Holyoke_Giveaway_Addresses |> 
  st_write("input_data/Holyoke/Holyoke_Giveaway_Addresses.gpkg")

```



